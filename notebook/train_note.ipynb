{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0391092d",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8cd1e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /opt/tensorflow/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/tensorflow/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/tensorflow/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/tensorflow/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/tensorflow/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/tensorflow/lib/python3.10/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/tensorflow/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/tensorflow/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/tensorflow/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/tensorflow/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tensorflow/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/tensorflow/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tensorflow/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.3-py3-none-any.whl (447 kB)\n",
      "Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.26.3 regex-2024.11.6 safetensors-0.4.5 tokenizers-0.20.3 transformers-4.46.3\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d72c9d",
   "metadata": {},
   "source": [
    "## Read data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "826afa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a471be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SSM client\n",
    "ssm = boto3.client('ssm', region_name=\"us-east-1\")\n",
    "\n",
    "# Fetch the parameter value\n",
    "response = ssm.get_parameter(\n",
    "    Name='/kaggle/bucket_name',\n",
    "    WithDecryption=False\n",
    ")\n",
    "\n",
    "bucket_name = response['Parameter']['Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "925fed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f3fbbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_from_s3(bucket_name, file_name):\n",
    "    obj = s3.get_object(Bucket = bucket_name, Key = file_name)\n",
    "    data = obj['Body'].read().decode('utf-8')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c912375",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"arrange/eng_news_2023_1M/eng_news_2023_1M/eng_news_2023_1M-sentences.txt\"\n",
    "file_content = read_file_from_s3(bucket_name, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6349cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = [line.split(\"\\t\")[1].lower().strip() for line in file_content.splitlines()]\n",
    "input_texts = [x.translate(str.maketrans('', '', string.punctuation)) for x in cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6406b84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'after a 20year battle with debilitating arthritis dunedin man bill napier was willing to try anything'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts[16985]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f08e677",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f49fc6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 10:39:02.556374: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-05 10:39:13.535594: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-05 10:39:13.636179: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-05 10:39:28.770364: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-05 10:39:44.024994: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db07565d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_texts \u001b[38;5;241m=\u001b[39m \u001b[43minput_texts\u001b[49m[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m5000\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_texts' is not defined"
     ]
    }
   ],
   "source": [
    "input_texts = input_texts[0:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c00c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_texts_token = [x.split(\" \") for x in input_texts]\n",
    "target_texts = [\" \".join(random.sample(x, len(x))) for x in target_texts_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7080c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le tokenizer et le modèle BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa4bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_encodings = tokenizer(input_texts, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "output_encodings = tokenizer(target_texts, padding=True, truncation=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a585c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_encodings[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1b1d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_encodings[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0e8738",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9fbf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c82a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSeq2Seq(tf.keras.Model):\n",
    "    def __init__(self, bert_model, vocab_size, hidden_size):\n",
    "        super(BertSeq2Seq, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.decoder = tf.keras.layers.LSTM(hidden_size, return_sequences=True, return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_ids, attention_mask = inputs[\"input_ids\"], inputs[\"attention_mask\"]\n",
    "        # Encode with BERT\n",
    "        encoder_output = self.bert(input_ids, attention_mask=attention_mask)[0]\n",
    "\n",
    "        # Decode\n",
    "        decoder_output, _, _ = self.decoder(encoder_output)\n",
    "        output = self.dense(decoder_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "433c6dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dbbc0499a8343a1bfe1f9af17653554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3637982129848169f3a057fbc94af60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 10:40:23.018295: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m bert_model \u001b[38;5;241m=\u001b[39m TFBertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Instantiate the seq2seq model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m      6\u001b[0m hidden_size \u001b[38;5;241m=\u001b[39m bert_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m BertSeq2Seq(bert_model, vocab_size, hidden_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model\n",
    "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Instantiate the seq2seq model\n",
    "vocab_size = tokenizer.vocab_size\n",
    "hidden_size = bert_model.config.hidden_size\n",
    "model = BertSeq2Seq(bert_model, vocab_size, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f74378c",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e2e91c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m bert_model \u001b[38;5;241m=\u001b[39m TFBertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Instantiate the seq2seq model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m      6\u001b[0m hidden_size \u001b[38;5;241m=\u001b[39m bert_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m BertSeq2Seq(bert_model, vocab_size, hidden_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model\n",
    "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Instantiate the seq2seq model\n",
    "vocab_size = tokenizer.vocab_size\n",
    "hidden_size = bert_model.config.hidden_size\n",
    "model = BertSeq2Seq(bert_model, vocab_size, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe237f4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_encodings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43minput_encodings\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_encodings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m }\n\u001b[1;32m      5\u001b[0m labels \u001b[38;5;241m=\u001b[39m output_encodings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_encodings' is not defined"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"input_ids\": input_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": input_encodings[\"attention_mask\"]\n",
    "}\n",
    "labels = output_encodings[\"input_ids\"]\n",
    "\n",
    "# Train the model\n",
    "model.fit(inputs,\n",
    "          labels,\n",
    "          batch_size=64,\n",
    "          epochs=3,\n",
    "          validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e490bb7a",
   "metadata": {},
   "source": [
    " ## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29df102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(model, tokenizer, input_sentence, max_length=20):\n",
    "    inputs = tokenizer(input_sentence, return_tensors=\"tf\", padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "    prediction = model.predict({\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"]})\n",
    "    predicted_ids = tf.argmax(prediction, axis=-1).numpy()\n",
    "    return tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50fdc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "jumbled_sentence = \"sleeps cat under tree the\"\n",
    "corrected_sentence = predict_sentence(model, tokenizer, jumbled_sentence)\n",
    "print(\"Reordered Sentence:\", corrected_sentence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

